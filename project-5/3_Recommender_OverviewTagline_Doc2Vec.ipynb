{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender: Overview of Movie Synopsis\n",
    "\n",
    "This notebook uses the 'overview' column, which is concise description of the movie synopsis, to make a recommender. It uses a TfidfVectorizer to capture infrequently used words and weigh them more heavily (in contrast to CountVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### IMPORTS ###\n",
    "###############\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn import svm\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.tag import pos_tag\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: http://zwmiller.com/projects/nlp_pipeline.html\n",
    "# Reference: https://github.com/ZWMiller/nlp_pipe_manager/blob/master/nlp_pipeline_manager/nlp_preprocessor.py\n",
    "# Reference: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "class nlp_pipe:\n",
    "    \n",
    "    # Initialize the class\n",
    "    def __init__(self, vectorizer, stemmer, lemmatizer, tokenizer, dataframe, column='Title'):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.stemmer = stemmer\n",
    "        self.dataframe = dataframe\n",
    "        self.column = column\n",
    "        self.dataframe[self.column] = self.dataframe[self.column].apply(str)\n",
    "    \n",
    "    ######################################################################\n",
    "    \n",
    "    # Create a cleaning method (aka fit) that will use several functions in order\n",
    "    def cleaner(self):\n",
    "        self.vader_sentiment()\n",
    "        self.dataframe = self._remove_numbers(self.dataframe, self.column)\n",
    "        self.dataframe = self._punctuation(self.dataframe, self.column)\n",
    "        self.dataframe = self._dropduplicates(self.dataframe, self.column)\n",
    "        self.real_words() # Check if it's a real word and then remove if not\n",
    "        #self.remove_single_letter() # Remove single letter words\n",
    "        #self.autocorrect() # Takes a very long time to run\n",
    "        self.tokenize_words()\n",
    "        self.remove_short_headlines() # Remove headline if only one word\n",
    "        #self.lemmatize_words()\n",
    "        #self.stem_words()\n",
    "        #self.named_entities()\n",
    "        self.dataframe = self._join_words(self.dataframe, self.column)\n",
    "        #self.dataframe[self.column] = self.dataframe[self.column].replace('', np.nan,)\n",
    "        #self.dataframe.dropna(subset=[self.column], inplace=True)\n",
    "    \n",
    "    ########## Functions that 'cleaner' will call ##########\n",
    "    @staticmethod\n",
    "    def _remove_numbers(dataframe, column):       \n",
    "        # Removes all words containing numbers\n",
    "        remove_numbers = lambda x: re.sub('\\w*\\d\\w*', '', x)\n",
    "        dataframe[column] = dataframe[column].map(remove_numbers)\n",
    "        return dataframe\n",
    "        \n",
    "    @staticmethod\n",
    "    def _punctuation(dataframe, column):\n",
    "        # Removes punctuation marks\n",
    "        punc_lower = lambda x: re.sub('[^A-Za-z0-9]+', ' ', x)\n",
    "        dataframe[column] = dataframe[column].map(punc_lower)\n",
    "        return dataframe\n",
    "        \n",
    "    @staticmethod\n",
    "    def _dropduplicates(dataframe, column):\n",
    "        # Drop rows that have duplicate 'Titles'\n",
    "        dataframe.drop_duplicates(subset=column, keep='first', inplace=True)\n",
    "        return dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def _join_words(dataframe, column):\n",
    "        # Joins words together with space (' ')--used after tokenization\n",
    "        join_words = lambda x: ' '.join(x)\n",
    "        dataframe[column] = dataframe[column].map(join_words)\n",
    "        return dataframe\n",
    "    \n",
    "    def vader_sentiment(self):\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        self.dataframe['Positive_Sentiment'] = self.dataframe.apply(lambda x: analyzer.polarity_scores(x[self.column])['pos'], axis=1)\n",
    "        self.dataframe['Negative_Sentiment'] = self.dataframe.apply(lambda x: analyzer.polarity_scores(x[self.column])['neg'], axis=1)\n",
    "        self.dataframe['Neutral_Sentiment'] = self.dataframe.apply(lambda x: analyzer.polarity_scores(x[self.column])['neu'], axis=1)\n",
    "        self.dataframe['Compound_Sentiment'] = self.dataframe.apply(lambda x: analyzer.polarity_scores(x[self.column])['compound'], axis=1)\n",
    "        \n",
    "    def tokenize_words(self):\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: self.tokenizer(x[self.column]), axis=1)\n",
    "    \n",
    "    def stem_words(self):\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: [self.stemmer.stem(word) for word in x[self.column]], axis=1)\n",
    "                                                           \n",
    "    def lemmatize_words(self):\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: [self.lemmatizer.lemmatize(word) for word in x[self.column]], axis=1)\n",
    "        \n",
    "    def named_entities(self):\n",
    "        st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                               '/usr/share/stanford-ner/stanford-ner.jar',\n",
    "                               encoding='utf-8')\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: st.tag(x[self.column]), axis=1)\n",
    "        \n",
    "    def real_words(self):\n",
    "        # Removes words that are not within the nltk.corpus library\n",
    "        words = set(nltk.corpus.words.words())\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: \\\n",
    "        \" \".join(w for w in nltk.wordpunct_tokenize(x[self.column]) if w.lower() in words or not w.isalpha()), axis=1)\n",
    "        \n",
    "    def remove_single_letter(self):\n",
    "        # Removes words that are 1 letter\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: ' '.join([w for w in x[self.column].split() if len(w)>2]), axis=1)\n",
    "        \n",
    "    def remove_short_headlines(self, min_length=3):\n",
    "        # Removes headlines that are less than 3 words\n",
    "        self.dataframe['headline_length'] = self.dataframe.apply(lambda x: len(x[self.column]), axis=1)\n",
    "        self.dataframe = self.dataframe[self.dataframe['headline_length'] > min_length]\n",
    "        self.dataframe = self.dataframe.drop(columns='headline_length')\n",
    "        self.dataframe.reset_index(drop=True)\n",
    "        \n",
    "    def remove_headlines_specific_words(self):\n",
    "        self.dataframe = self.dataframe[~self.dataframe[self.column].str.contains('onion')]\n",
    "        self.dataframe = self.dataframe[~self.dataframe[self.column].str.contains('Onion')]\n",
    "        \n",
    "    def autocorrect(self):\n",
    "        # Autocorrects words based on Levenshtein distance (takes __ minutes to run)\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: ''.join(TextBlob(x[self.column]).correct()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('data/dataframe_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe:  (46628, 28)\n",
      "Columns of dataframe:  Index(['adult', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'id',\n",
      "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
      "       'popularity', 'poster_path', 'production_companies',\n",
      "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
      "       'spoken_languages', 'status', 'tagline', 'title', 'video',\n",
      "       'vote_average', 'vote_count', 'cast', 'crew', 'keywords', 'director'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataframe: ', df_all.shape)\n",
    "print('Columns of dataframe: ', df_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataframe_merged_small.csv', usecols=['id', 'title', 'overview', 'tagline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe:  (10876, 4)\n",
      "Columns of dataframe:  Index(['id', 'overview', 'tagline', 'title'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataframe: ', df.shape)\n",
    "print('Columns of dataframe: ', df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949</td>\n",
       "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
       "      <td>A Los Angeles Crime Saga</td>\n",
       "      <td>Heat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11860</td>\n",
       "      <td>An ugly duckling having undergone a remarkable...</td>\n",
       "      <td>You are cordially invited to the most surprisi...</td>\n",
       "      <td>Sabrina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>19307</td>\n",
       "      <td>Sid and Bernie keep having their amorous inten...</td>\n",
       "      <td>Fun and games in the great outdoors!</td>\n",
       "      <td>Carry On Camping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>18098</td>\n",
       "      <td>Scheherezade puts herself in danger to save Su...</td>\n",
       "      <td>When Night Falls, the Adventure Begins!</td>\n",
       "      <td>Arabian Nights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>52103</td>\n",
       "      <td>Little pocket thief Wu never got away from the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pickpocket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>455661</td>\n",
       "      <td>A closeted boy runs the risk of being outed by...</td>\n",
       "      <td>The Heart Wants What The Heart Wants</td>\n",
       "      <td>In a Heartbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>49279</td>\n",
       "      <td>A chemist in his laboratory places upon a tabl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Man with the Rubber Head</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           overview  \\\n",
       "0         862  Led by Woody, Andy's toys live happily in his ...   \n",
       "1        8844  When siblings Judy and Peter discover an encha...   \n",
       "2       15602  A family wedding reignites the ancient feud be...   \n",
       "3         949  Obsessive master thief, Neil McCauley leads a ...   \n",
       "4       11860  An ugly duckling having undergone a remarkable...   \n",
       "...       ...                                                ...   \n",
       "10871   19307  Sid and Bernie keep having their amorous inten...   \n",
       "10872   18098  Scheherezade puts herself in danger to save Su...   \n",
       "10873   52103  Little pocket thief Wu never got away from the...   \n",
       "10874  455661  A closeted boy runs the risk of being outed by...   \n",
       "10875   49279  A chemist in his laboratory places upon a tabl...   \n",
       "\n",
       "                                                 tagline  \\\n",
       "0                                                    NaN   \n",
       "1              Roll the dice and unleash the excitement!   \n",
       "2      Still Yelling. Still Fighting. Still Ready for...   \n",
       "3                               A Los Angeles Crime Saga   \n",
       "4      You are cordially invited to the most surprisi...   \n",
       "...                                                  ...   \n",
       "10871               Fun and games in the great outdoors!   \n",
       "10872            When Night Falls, the Adventure Begins!   \n",
       "10873                                                NaN   \n",
       "10874               The Heart Wants What The Heart Wants   \n",
       "10875                                                NaN   \n",
       "\n",
       "                              title  \n",
       "0                         Toy Story  \n",
       "1                           Jumanji  \n",
       "2                  Grumpier Old Men  \n",
       "3                              Heat  \n",
       "4                           Sabrina  \n",
       "...                             ...  \n",
       "10871              Carry On Camping  \n",
       "10872                Arabian Nights  \n",
       "10873                    Pickpocket  \n",
       "10874                In a Heartbeat  \n",
       "10875  The Man with the Rubber Head  \n",
       "\n",
       "[10876 rows x 4 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with empty strings\n",
    "df['overview'] = df['overview'].replace(np.nan, '', regex=True)\n",
    "df['tagline'] = df['tagline'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join [overview] and [keywords] together\n",
    "# These two columns are synopsis-associated and it's sensible to join them together\n",
    "df['overview_and_tagline'] = df['overview'] + df['tagline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overview_and_tagline'].loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text using NLP class functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text using nlp_pipelines class\n",
    "nlp = nlp_pipe(dataframe = df,\n",
    "               column = 'overview_and_tagline',\n",
    "               tokenizer = nltk.word_tokenize,\n",
    "               vectorizer = TfidfVectorizer(stop_words='english'),\n",
    "               stemmer = SnowballStemmer(\"english\"),\n",
    "               lemmatizer = WordNetLemmatizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.cleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>overview_and_tagline</th>\n",
       "      <th>Positive_Sentiment</th>\n",
       "      <th>Negative_Sentiment</th>\n",
       "      <th>Neutral_Sentiment</th>\n",
       "      <th>Compound_Sentiment</th>\n",
       "      <th>headline_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td></td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>[Led, Woody, live, happily, his, room, until, ...</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>[When, and, Peter, discover, board, game, that...</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>[family, wedding, the, ancient, feud, between,...</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.7105</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949</td>\n",
       "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
       "      <td>A Los Angeles Crime Saga</td>\n",
       "      <td>Heat</td>\n",
       "      <td>[Obsessive, master, thief, top, notch, crew, v...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.725</td>\n",
       "      <td>-0.8555</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11860</td>\n",
       "      <td>An ugly duckling having undergone a remarkable...</td>\n",
       "      <td>You are cordially invited to the most surprisi...</td>\n",
       "      <td>Sabrina</td>\n",
       "      <td>[ugly, duckling, remarkable, change, still, fo...</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>19307</td>\n",
       "      <td>Sid and Bernie keep having their amorous inten...</td>\n",
       "      <td>Fun and games in the great outdoors!</td>\n",
       "      <td>Carry On Camping</td>\n",
       "      <td>[and, keep, their, amorous, snubbed, their, an...</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.9472</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>18098</td>\n",
       "      <td>Scheherezade puts herself in danger to save Su...</td>\n",
       "      <td>When Night Falls, the Adventure Begins!</td>\n",
       "      <td>Arabian Nights</td>\n",
       "      <td>[herself, danger, save, Sultan, her, childhood...</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-0.7500</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>52103</td>\n",
       "      <td>Little pocket thief Wu never got away from the...</td>\n",
       "      <td></td>\n",
       "      <td>Pickpocket</td>\n",
       "      <td>[Little, pocket, thief, never, got, away, from...</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.664</td>\n",
       "      <td>-0.2882</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>455661</td>\n",
       "      <td>A closeted boy runs the risk of being outed by...</td>\n",
       "      <td>The Heart Wants What The Heart Wants</td>\n",
       "      <td>In a Heartbeat</td>\n",
       "      <td>[boy, the, risk, being, outed, his, own, heart...</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>49279</td>\n",
       "      <td>A chemist in his laboratory places upon a tabl...</td>\n",
       "      <td></td>\n",
       "      <td>The Man with the Rubber Head</td>\n",
       "      <td>[chemist, his, laboratory, upon, table, his, o...</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10632 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           overview  \\\n",
       "0         862  Led by Woody, Andy's toys live happily in his ...   \n",
       "1        8844  When siblings Judy and Peter discover an encha...   \n",
       "2       15602  A family wedding reignites the ancient feud be...   \n",
       "3         949  Obsessive master thief, Neil McCauley leads a ...   \n",
       "4       11860  An ugly duckling having undergone a remarkable...   \n",
       "...       ...                                                ...   \n",
       "10871   19307  Sid and Bernie keep having their amorous inten...   \n",
       "10872   18098  Scheherezade puts herself in danger to save Su...   \n",
       "10873   52103  Little pocket thief Wu never got away from the...   \n",
       "10874  455661  A closeted boy runs the risk of being outed by...   \n",
       "10875   49279  A chemist in his laboratory places upon a tabl...   \n",
       "\n",
       "                                                 tagline  \\\n",
       "0                                                          \n",
       "1              Roll the dice and unleash the excitement!   \n",
       "2      Still Yelling. Still Fighting. Still Ready for...   \n",
       "3                               A Los Angeles Crime Saga   \n",
       "4      You are cordially invited to the most surprisi...   \n",
       "...                                                  ...   \n",
       "10871               Fun and games in the great outdoors!   \n",
       "10872            When Night Falls, the Adventure Begins!   \n",
       "10873                                                      \n",
       "10874               The Heart Wants What The Heart Wants   \n",
       "10875                                                      \n",
       "\n",
       "                              title  \\\n",
       "0                         Toy Story   \n",
       "1                           Jumanji   \n",
       "2                  Grumpier Old Men   \n",
       "3                              Heat   \n",
       "4                           Sabrina   \n",
       "...                             ...   \n",
       "10871              Carry On Camping   \n",
       "10872                Arabian Nights   \n",
       "10873                    Pickpocket   \n",
       "10874                In a Heartbeat   \n",
       "10875  The Man with the Rubber Head   \n",
       "\n",
       "                                    overview_and_tagline  Positive_Sentiment  \\\n",
       "0      [Led, Woody, live, happily, his, room, until, ...               0.091   \n",
       "1      [When, and, Peter, discover, board, game, that...               0.161   \n",
       "2      [family, wedding, the, ancient, feud, between,...               0.157   \n",
       "3      [Obsessive, master, thief, top, notch, crew, v...               0.068   \n",
       "4      [ugly, duckling, remarkable, change, still, fo...               0.158   \n",
       "...                                                  ...                 ...   \n",
       "10871  [and, keep, their, amorous, snubbed, their, an...               0.217   \n",
       "10872  [herself, danger, save, Sultan, her, childhood...               0.171   \n",
       "10873  [Little, pocket, thief, never, got, away, from...               0.145   \n",
       "10874  [boy, the, risk, being, outed, his, own, heart...               0.282   \n",
       "10875  [chemist, his, laboratory, upon, table, his, o...               0.076   \n",
       "\n",
       "       Negative_Sentiment  Neutral_Sentiment  Compound_Sentiment  \\\n",
       "0                   0.034              0.875              0.4767   \n",
       "1                   0.144              0.695              0.1260   \n",
       "2                   0.132              0.710              0.7105   \n",
       "3                   0.207              0.725             -0.8555   \n",
       "4                   0.076              0.766              0.5844   \n",
       "...                   ...                ...                 ...   \n",
       "10871               0.048              0.735              0.9472   \n",
       "10872               0.277              0.551             -0.7500   \n",
       "10873               0.191              0.664             -0.2882   \n",
       "10874               0.047              0.671              0.9100   \n",
       "10875               0.063              0.861              0.1779   \n",
       "\n",
       "       headline_length  \n",
       "0                   35  \n",
       "1                   55  \n",
       "2                   45  \n",
       "3                   43  \n",
       "4                   28  \n",
       "...                ...  \n",
       "10871               46  \n",
       "10872               24  \n",
       "10873               37  \n",
       "10874               22  \n",
       "10875               81  \n",
       "\n",
       "[10632 rows x 10 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the word2vec model and train on df['overview_and_tagline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tag train set\n",
    "tagged_tr = [TaggedDocument(words=word_tokenize(_d.lower()),\n",
    "tags = [str(i)]) for i, _d in enumerate(train.overview_and_tagline)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tag test set\n",
    "tagged_test = [TaggedDocument(words=word_tokenize(_d.lower()),\n",
    "tags=[str(i)]) for i, _d in enumerate(test.overview_and_tagline)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=100,\n",
    "                window=5, \n",
    "                alpha=.025, \n",
    "                min_alpha=0.00025, \n",
    "                min_count=2, \n",
    "                dm=1, \n",
    "                workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n",
      "Epoch 51\n",
      "Epoch 52\n",
      "Epoch 53\n",
      "Epoch 54\n",
      "Epoch 55\n",
      "Epoch 56\n",
      "Epoch 57\n",
      "Epoch 58\n",
      "Epoch 59\n",
      "Epoch 60\n",
      "Epoch 61\n",
      "Epoch 62\n",
      "Epoch 63\n",
      "Epoch 64\n",
      "Epoch 65\n",
      "Epoch 66\n",
      "Epoch 67\n",
      "Epoch 68\n",
      "Epoch 69\n",
      "Epoch 70\n",
      "Epoch 71\n",
      "Epoch 72\n",
      "Epoch 73\n",
      "Epoch 74\n",
      "Epoch 75\n",
      "Epoch 76\n",
      "Epoch 77\n",
      "Epoch 78\n",
      "Epoch 79\n",
      "Epoch 80\n",
      "Epoch 81\n",
      "Epoch 82\n",
      "Epoch 83\n",
      "Epoch 84\n",
      "Epoch 85\n",
      "Epoch 86\n",
      "Epoch 87\n",
      "Epoch 88\n",
      "Epoch 89\n",
      "Epoch 90\n",
      "Epoch 91\n",
      "Epoch 92\n",
      "Epoch 93\n",
      "Epoch 94\n",
      "Epoch 95\n",
      "Epoch 96\n",
      "Epoch 97\n",
      "Epoch 98\n",
      "Epoch 99\n",
      "Epoch 100\n"
     ]
    }
   ],
   "source": [
    "epochs = range(100)\n",
    "for epoch in epochs:\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    model.train(tagged_tr,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.00025\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "     \n",
    "model.save('data/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([model.docvecs[str(i)] for i in range(len(tagged_tr))])\n",
    "\n",
    "y_train = train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8491     250124\n",
       "7178     119431\n",
       "2423       9026\n",
       "10612    283591\n",
       "10791     34933\n",
       "          ...  \n",
       "5734      15801\n",
       "5191       2009\n",
       "5390      12473\n",
       "860         887\n",
       "7270      77117\n",
       "Name: id, Length: 8700, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lady', 0.4496452808380127), ('prisoners', 0.41911080479621887), ('woman', 0.4177355468273163), ('boy', 0.38012969493865967), ('ensures', 0.3707790672779083), ('doctor', 0.363141804933548), ('dickinson', 0.36013805866241455), ('servicing', 0.35690784454345703), ('soldier', 0.3567981719970703), ('mankind', 0.35655438899993896)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-7f3f2f207019>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  print(model.most_similar('warrior'))\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar('warrior'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5275', 0.3733769953250885), ('6734', 0.3408944606781006), ('2676', 0.31792134046554565), ('7098', 0.3138238787651062), ('6990', 0.3135916590690613), ('7619', 0.31078362464904785), ('4977', 0.3083052635192871), ('2337', 0.3060140907764435), ('7395', 0.2977234423160553), ('5165', 0.2973651885986328)]\n"
     ]
    }
   ],
   "source": [
    "# Find movies similar to query word\n",
    "vec = model['warrior']\n",
    "print(model.docvecs.most_similar([vec]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-19c6b5688b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Toy Story'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "new_vector = model.infer_vector(tokens)\n",
    "model.docvecs.most_similar([new_vector]) #gives you top 10 document tags and their cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_word = vectorizer.fit_transform(df['overview_and_keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity matrix from doc_word\n",
    "cosine_sim = cosine_similarity(doc_word, doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10866</th>\n",
       "      <th>10867</th>\n",
       "      <th>10868</th>\n",
       "      <th>10869</th>\n",
       "      <th>10870</th>\n",
       "      <th>10871</th>\n",
       "      <th>10872</th>\n",
       "      <th>10873</th>\n",
       "      <th>10874</th>\n",
       "      <th>10875</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010031</td>\n",
       "      <td>0.046185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040562</td>\n",
       "      <td>0.052569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.050754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.032332</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.016196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>0.010031</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>0.050754</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.026808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>0.004446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>0.046185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.062217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows × 10876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6      \\\n",
       "0      1.000000  0.013389  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1      0.013389  1.000000  0.040562  0.052569  0.000000  0.000000  0.009913   \n",
       "2      0.000000  0.040562  1.000000  0.000000  0.000000  0.000000  0.010711   \n",
       "3      0.000000  0.052569  0.000000  1.000000  0.000000  0.000000  0.000000   \n",
       "4      0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10871  0.000000  0.000000  0.000000  0.000000  0.016196  0.000000  0.000000   \n",
       "10872  0.000000  0.000000  0.016014  0.000000  0.000000  0.000000  0.000000   \n",
       "10873  0.010031  0.047648  0.050754  0.028662  0.000000  0.000000  0.008306   \n",
       "10874  0.046185  0.000000  0.000000  0.032332  0.000000  0.000000  0.009476   \n",
       "10875  0.000000  0.012675  0.014073  0.000000  0.000000  0.072149  0.000000   \n",
       "\n",
       "       7         8         9      ...  10866     10867     10868     10869  \\\n",
       "0        0.0  0.000000  0.000000  ...    0.0  0.000000  0.000000  0.000000   \n",
       "1        0.0  0.005114  0.000000  ...    0.0  0.000000  0.000000  0.000000   \n",
       "2        0.0  0.000000  0.000000  ...    0.0  0.000000  0.000000  0.000000   \n",
       "3        0.0  0.018957  0.000000  ...    0.0  0.000000  0.000000  0.000000   \n",
       "4        0.0  0.000000  0.000000  ...    0.0  0.000000  0.000000  0.000000   \n",
       "...      ...       ...       ...  ...    ...       ...       ...       ...   \n",
       "10871    0.0  0.000000  0.040168  ...    0.0  0.031422  0.000000  0.000000   \n",
       "10872    0.0  0.000000  0.000000  ...    0.0  0.000000  0.000000  0.000000   \n",
       "10873    0.0  0.000000  0.000000  ...    0.0  0.000000  0.000000  0.000000   \n",
       "10874    0.0  0.000000  0.000000  ...    0.0  0.000000  0.034507  0.062217   \n",
       "10875    0.0  0.000000  0.000000  ...    0.0  0.000000  0.000000  0.022442   \n",
       "\n",
       "          10870     10871     10872     10873     10874     10875  \n",
       "0      0.000000  0.000000  0.000000  0.010031  0.046185  0.000000  \n",
       "1      0.003447  0.000000  0.000000  0.047648  0.000000  0.012675  \n",
       "2      0.016470  0.000000  0.016014  0.050754  0.000000  0.014073  \n",
       "3      0.000000  0.000000  0.000000  0.028662  0.032332  0.000000  \n",
       "4      0.009021  0.016196  0.000000  0.000000  0.000000  0.000000  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "10871  0.007882  1.000000  0.000000  0.026808  0.000000  0.000000  \n",
       "10872  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  \n",
       "10873  0.005253  0.026808  0.000000  1.000000  0.013248  0.004446  \n",
       "10874  0.000000  0.000000  0.000000  0.013248  1.000000  0.013403  \n",
       "10875  0.000000  0.000000  0.000000  0.004446  0.013403  1.000000  \n",
       "\n",
       "[10876 rows x 10876 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the cosine_sim matrix\n",
    "pd.DataFrame(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save cosine_sim array to use in hybrid recommendation system\n",
    "np.save('similarity_matrix/cos_overview_small.npy', cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index of our dataframe and construct reverse mapping as before\n",
    "indices = pd.Series(df.index, index=df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5365                                       The Dark Knight\n",
       "274                                                 Batman\n",
       "667                                         Batman Returns\n",
       "6209                            Batman: Under the Red Hood\n",
       "2707                                                 Q & A\n",
       "4411                                         Batman Begins\n",
       "4080                    Batman Beyond: Return of the Joker\n",
       "7450     Batman Unmasked: The Psychology of the Dark Kn...\n",
       "10350    LEGO DC Comics Super Heroes: Justice League - ...\n",
       "6752                                      Batman: Year One\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6179               Toy Story 3\n",
       "1458               Toy Story 2\n",
       "4470    The 40 Year Old Virgin\n",
       "8125                 Small Fry\n",
       "485      Rebel Without a Cause\n",
       "5007              Factory Girl\n",
       "2616             Class of 1984\n",
       "7273             A Simple Life\n",
       "1486           Man on the Moon\n",
       "1528      White Men Can't Jump\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Toy Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
