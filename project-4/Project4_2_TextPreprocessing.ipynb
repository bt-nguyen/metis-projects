{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn import svm\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.tag import pos_tag\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Workflow should be as follows:\n",
    "1. Read in data frame\n",
    "2. Lowercase all letters (regex)\n",
    "3. Address punctutation marks (regex)\n",
    "?. Remove non-english titles (?)\n",
    "4. Break headlines into single words (tokenize)\n",
    "5. Find and remove/minimize words that are for semantics (stop words)\n",
    "6. Find similar words and bin together (stemming)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: http://zwmiller.com/projects/nlp_pipeline.html\n",
    "# Reference: https://github.com/ZWMiller/nlp_pipe_manager/blob/master/nlp_pipeline_manager/nlp_preprocessor.py\n",
    "# Reference: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "class nlp_pipe:\n",
    "    \n",
    "    # Initialize the class\n",
    "    def __init__(self, vectorizer, stemmer, lemmatizer, tokenizer, dataframe, column='Title'):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.stemmer = stemmer\n",
    "        self.dataframe = dataframe\n",
    "        self.column = column\n",
    "        self.dataframe[self.column] = self.dataframe[self.column].apply(str)\n",
    "    \n",
    "    ######################################################################\n",
    "    \n",
    "    # Create a cleaning method (aka fit) that will use several functions in order\n",
    "    def cleaner(self):\n",
    "        self.vader_sentiment()\n",
    "        self.dataframe = self._remove_numbers(self.dataframe, self.column)\n",
    "        self.dataframe = self._punctuation(self.dataframe, self.column)\n",
    "        self.dataframe = self._dropduplicates(self.dataframe, self.column)\n",
    "        self.real_words() # Check if it's a real word and then remove if not\n",
    "        self.remove_single_letter() # Remove single letter words\n",
    "        #self.autocorrect() # Takes a very long time to run\n",
    "        self.tokenize_words()\n",
    "        self.remove_short_headlines() # Remove headline if only one word\n",
    "        #self.lemmatize_words()\n",
    "        #self.stem_words()\n",
    "        #self.named_entities()\n",
    "        self.dataframe = self._join_words(self.dataframe, self.column)\n",
    "        self.remove_headlines_specific_words()\n",
    "        self.dataframe[self.column] = self.dataframe[self.column].replace('', np.nan,)\n",
    "        self.dataframe.dropna(subset=[self.column], inplace=True)\n",
    "    \n",
    "    ########## Functions that 'cleaner' will call ##########\n",
    "    @staticmethod\n",
    "    def _remove_numbers(dataframe, column):       \n",
    "        # Removes all words containing numbers\n",
    "        remove_numbers = lambda x: re.sub('\\w*\\d\\w*', '', x)\n",
    "        dataframe[column] = dataframe[column].map(remove_numbers)\n",
    "        return dataframe\n",
    "        \n",
    "    @staticmethod\n",
    "    def _punctuation(dataframe, column):\n",
    "        # Removes punctuation marks\n",
    "        punc_lower = lambda x: re.sub('[^A-Za-z0-9]+', ' ', x)\n",
    "        dataframe[column] = dataframe[column].map(punc_lower)\n",
    "        return dataframe\n",
    "        \n",
    "    @staticmethod\n",
    "    def _dropduplicates(dataframe, column):\n",
    "        # Drop rows that have duplicate 'Titles'\n",
    "        dataframe.drop_duplicates(subset=column, keep='first', inplace=True)\n",
    "        return dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def _join_words(dataframe, column):\n",
    "        # Joins words together with space (' ')--used after tokenization\n",
    "        join_words = lambda x: ' '.join(x)\n",
    "        dataframe[column] = dataframe[column].map(join_words)\n",
    "        return dataframe\n",
    "    \n",
    "    def vader_sentiment(self):\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        self.dataframe['Positive_Sentiment'] = self.dataframe.apply(lambda x: analyzer.polarity_scores(x[self.column])['pos'], axis=1)\n",
    "        self.dataframe['Negative_Sentiment'] = self.dataframe.apply(lambda x: analyzer.polarity_scores(x[self.column])['neg'], axis=1)\n",
    "        self.dataframe['Neutral_Sentiment'] = self.dataframe.apply(lambda x: analyzer.polarity_scores(x[self.column])['neu'], axis=1)\n",
    "        self.dataframe['Compound_Sentiment'] = self.dataframe.apply(lambda x: analyzer.polarity_scores(x[self.column])['compound'], axis=1)\n",
    "        \n",
    "    def tokenize_words(self):\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: self.tokenizer(x[self.column]), axis=1)\n",
    "    \n",
    "    def stem_words(self):\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: [self.stemmer.stem(word) for word in x[self.column]], axis=1)\n",
    "                                                           \n",
    "    def lemmatize_words(self):\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: [self.lemmatizer.lemmatize(word) for word in x[self.column]], axis=1)\n",
    "        \n",
    "    def named_entities(self):\n",
    "        st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                               '/usr/share/stanford-ner/stanford-ner.jar',\n",
    "                               encoding='utf-8')\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: st.tag(x[self.column]), axis=1)\n",
    "        \n",
    "    def real_words(self):\n",
    "        # Removes words that are not within the nltk.corpus library\n",
    "        words = set(nltk.corpus.words.words())\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: \\\n",
    "        \" \".join(w for w in nltk.wordpunct_tokenize(x[self.column]) if w.lower() in words or not w.isalpha()), axis=1)\n",
    "        \n",
    "    def remove_single_letter(self):\n",
    "        # Removes words that are 1 letter\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: ' '.join([w for w in x[self.column].split() if len(w)>2]), axis=1)\n",
    "        \n",
    "    def remove_short_headlines(self, min_length=3):\n",
    "        # Removes headlines that are less than 3 words\n",
    "        self.dataframe['headline_length'] = self.dataframe.apply(lambda x: len(x[self.column]), axis=1)\n",
    "        self.dataframe = self.dataframe[self.dataframe['headline_length'] > min_length]\n",
    "        self.dataframe = self.dataframe.drop(columns='headline_length')\n",
    "        self.dataframe.reset_index(drop=True)\n",
    "        \n",
    "    def remove_headlines_specific_words(self):\n",
    "        self.dataframe = self.dataframe[~self.dataframe[self.column].str.contains('onion')]\n",
    "        self.dataframe = self.dataframe[~self.dataframe[self.column].str.contains('Onion')]\n",
    "        \n",
    "    def autocorrect(self):\n",
    "        # Autocorrects words based on Levenshtein distance (takes __ minutes to run)\n",
    "        self.dataframe[self.column] = self.dataframe.apply(lambda x: ''.join(TextBlob(x[self.column]).correct()), axis=1)\n",
    "\n",
    "        \n",
    "    ######################################################################\n",
    "\n",
    "    # Create a transform method (aka vectorization)\n",
    "    \n",
    "    def transform(self):\n",
    "        vect_series = self.vectorizer.fit_transform(self.dataframe[self.column])\n",
    "        self.dataframe_vect = pd.DataFrame(vect_series.toarray(), columns=self.vectorizer.get_feature_names())\n",
    "        \n",
    "    ######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1414"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/2020/onion_title_list_2020.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nlp_pipe(dataframe = df,\n",
    "               column = 'Title',\n",
    "               tokenizer = nltk.word_tokenize,\n",
    "               vectorizer = TfidfVectorizer(stop_words='english'),\n",
    "               stemmer = SnowballStemmer(\"english\"),\n",
    "               lemmatizer = WordNetLemmatizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.cleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.dataframe.to_csv('data/2020/onion_clean_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eac5ff8c14af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_onion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/2020/onion_clean_2020.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_notonion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/2020/notonion_clean_2020.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_onion = pd.read_csv('data/2020/onion_clean_2020.csv')\n",
    "df_notonion = pd.read_csv('data/2020/notonion_clean_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_onion, df_notonion]\n",
    "\n",
    "df_merge = pd.concat(frames)\n",
    "\n",
    "df_merge.to_csv('data/2020/merge_clean_2020.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
